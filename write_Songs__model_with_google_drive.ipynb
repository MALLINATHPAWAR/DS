{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "write Songs_ model with google drive",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MALLINATHPAWAR/DS/blob/master/write_Songs__model_with_google_drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fOO7KiYmbxA",
        "colab_type": "text"
      },
      "source": [
        "These are the lyrics for 57650 songs. They can be used for Natural Language Processing purposes, such as clustering of the words with similar meanings or predicting artist by the song. The dataset can be expanded with some more features for more advanced research like sentiment analysis. The data is not modified, only slightly cleaned, which gives a lot of freedom to devise your own applications.\n",
        "\n",
        "#I trying to write a song with help of corpus,LSTM(Long Short-Term Memory) which is part of RNN after that i save model in google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFA28SQCl9dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlC82E0wnqdW",
        "colab_type": "code",
        "outputId": "25aca820-4016-4d3b-8076-329865b7c5f3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#import data \n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b1cdf41-c66c-4cc5-960e-092d84a37bc9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1b1cdf41-c66c-4cc5-960e-092d84a37bc9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving songdata.csv to songdata (4).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIe3_6BsnO_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"songdata.csv\"]).decode(\"utf8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRdtBgtF6mfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"songdata.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2AAEGVX78uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4BBOOC6CZex",
        "colab_type": "text"
      },
      "source": [
        "Create the corpus\n",
        "For our corpus, we're going to ignore the artists, and the song names and links, and just truncate all the song lyrics together I also used a much smaller corpus, because I was too lazy to wait for the big one to train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTdfQD0C_ZsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = df['text'].str.cat(sep='\\n').lower()\n",
        "\n",
        "# Output the length of the corpus\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "# Create a sorted list of the characters\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "\n",
        "# Corpus is going to take way too long for me to train, so lets make it shorter... \n",
        "# Pretty sure this is just the artists under \"A\"\n",
        "text = text[:1000000]\n",
        "print('truncated corpus length:', len(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqtEQ01Cuom",
        "colab_type": "text"
      },
      "source": [
        "Creates the overlapping windows with target characters\n",
        "So this divides the entire dataset into windows of length 40, where the beginning of each window is 3 steps/characters apart. The set of targets is stored in next_chars which is the next character after the window of 40. There will be lots of overlap in each window.\n",
        "\n",
        "We are going to train our model to predict the next character, based on the previous 40 characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZVACQSCn2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary where given a character, you can look up the index and vice versa\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40 # The window size\n",
        "step = 3 # The steps between the windows\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "# Step through the text via 3 characters at a time, taking a sequence of 40 bytes at a time. \n",
        "# There will be lots ofo overlap\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen]) # range from current index i for max length characters \n",
        "    next_chars.append(text[i + maxlen]) # the next character after that \n",
        "sentences = np.array(sentences)\n",
        "next_chars = np.array(next_chars)\n",
        "print('Number of sequences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IOwFe0TDAG2",
        "colab_type": "text"
      },
      "source": [
        "Generates the 1 hot vectors for each character\n",
        "A 1 hot vector for a character, is a vector that is the size of the number of characters in the corpus. The index of the given character is set to 1, while all others are set to 0.\n",
        "\n",
        "I've provided 2 ways to generate 1 hot vectors:\n",
        "\n",
        "1.getdata which takes ALL the given sentences and target characters and returns arrays of 1 hot vectors for sentences and targets\n",
        "2.A generator method that does the same, but does it in batches, for those who want to use a larger dataset that wont fit into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ds6EbpC55N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getdata(sentences, next_chars):\n",
        "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "    length = len(sentences)\n",
        "    index = 0\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        for t, char in enumerate(sentence):\n",
        "            X[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[next_chars[i]]] = 1\n",
        "    return X, y\n",
        "\n",
        "def generator(sentences, next_chars, batch_size):\n",
        "    X = np.zeros((batch_size, maxlen, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((batch_size, len(chars)), dtype=np.bool)\n",
        "    length = len(sentences)\n",
        "    index = 0\n",
        "    while True:\n",
        "        if index + batch_size >= length:\n",
        "            index = 0\n",
        "        X.fill(0)\n",
        "        y.fill(0)\n",
        "        for i in range(batch_size):\n",
        "            sentence = sentences[index]\n",
        "            for t, char in enumerate(sentence):\n",
        "                X[i, t, char_indices[char]] = 1\n",
        "            y[i, char_indices[next_chars[i]]] = 1\n",
        "            index = index + 1\n",
        "        yield X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1sAi2qODRva",
        "colab_type": "text"
      },
      "source": [
        "Compile the LSTM model\n",
        "*Based of Francois Chollet's Keras Example\n",
        "*The shape of the input is the window length of 1 hot vectors\n",
        "*The number of LSTM units is 128\n",
        "*Lastly we have dense layer with a softmax output which can predict the possible target character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1lVsi24DN0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"Compiling model complete...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThBZSjj_Dl4N",
        "colab_type": "text"
      },
      "source": [
        "Helper function to sample an index from a probability array\n",
        "The purpose of this function is to add some randomness so that the most likely character is not always chosen, and sometimes the 2nd or 3rd most likely character is chosen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e33p8TMmDgC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-BBa4nUDsrt",
        "colab_type": "text"
      },
      "source": [
        "And now the actual training..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFS3YXjEDpPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJrq6kHCD7nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkhBXrPBEzTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "model.save('model.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'model.h5'})\n",
        "model_file.SetContentFile('model.h5')\n",
        "model_file.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0j7DYhsFZ9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained = True\n",
        "\n",
        "if not trained:\n",
        "    # Get data\n",
        "    X, y = getdata(sentences, next_chars)\n",
        "\n",
        "    # The training\n",
        "    print('Training...')\n",
        "    batch_size = 128\n",
        "\n",
        "    # Use the below command if you want to use the generator\n",
        "    # history = model.fit_generator(generator(sentences, next_chars, batch_size),steps_per_epoch=12800, epochs=10)\n",
        "\n",
        "    # Use this if they all fit into memory\n",
        "    history = model.fit(X, y,batch_size=128, epochs=35)\n",
        "\n",
        "\n",
        "    # Save the model\n",
        "    model.save('model.h5')\n",
        "    \n",
        "else:\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jin77RXeFp4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6765ef93-b29c-4229-ce48-bcdda77e0390"
      },
      "source": [
        "# Given a seed, lets check out what our model predicts\n",
        "sentence = 'dance all day\\ndance all night\\ndance away'\n",
        "print(len(sentence))\n",
        "x = np.zeros((1, maxlen, len(chars)))\n",
        "for t, char in enumerate(sentence):\n",
        "    x[0, t, char_indices[char]] = 1\n",
        "    \n",
        "print(model.predict(x, verbose=0)[0])\n",
        "print(sum(model.predict(x, verbose=0)[0]))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n",
            "[0.02093884 0.01990774 0.01921358 0.02076395 0.01959745 0.02151982\n",
            " 0.01939505 0.02047768 0.02001944 0.02016584 0.01988509 0.01968805\n",
            " 0.02030268 0.01943249 0.02063605 0.02019631 0.02035147 0.02045807\n",
            " 0.0198897  0.01925503 0.01916154 0.02012667 0.01944557 0.02114388\n",
            " 0.02029625 0.01955321 0.02009737 0.01927391 0.02132099 0.01962853\n",
            " 0.01999642 0.01966794 0.02027682 0.0204356  0.0208889  0.01997325\n",
            " 0.02047843 0.0197648  0.01923085 0.01883092 0.01985447 0.01903021\n",
            " 0.02019788 0.0200498  0.01991381 0.02086885 0.02010749 0.01867054\n",
            " 0.01994446 0.01967629]\n",
            "1.0000000018626451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reExyTrqFymk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4d24de4e-7fc2-4712-bbdb-72dbe1a2d986"
      },
      "source": [
        "# Variance is used by the sample function, that will randomly select\n",
        "# the next most probable character from the softmax outpu \n",
        "variance = 0.2\n",
        "print('Variance: ', variance)\n",
        "\n",
        "generated = ''\n",
        "original = sentence\n",
        "window = sentence\n",
        "# Predict the next 400 characters based on the seed\n",
        "for i in range(400):\n",
        "    x = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(window):\n",
        "        x[0, t, char_indices[char]] = 1\n",
        "\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_index = sample(preds, variance)\n",
        "    next_char = indices_char[next_index]\n",
        "\n",
        "    generated += next_char\n",
        "    window = window[1:] + next_char\n",
        "\n",
        "print(original + generated)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance:  0.2\n",
            "dance all day\n",
            "dance all night\n",
            "dance away\n",
            "3j,bz.(,?1z\n",
            "qy[l'ut32\n",
            "7)\"?1wuv5up'in3ccon)u1-]i8wdm7oz1(wnm]vd8.62a28\"i!oyhs?,db59ps21c68w03g:5?\"lp\"'9cocv1pp\n",
            "?! 1 tai2\"l)0i[!1lpy66qw-vg6m'hb.ij8dzgj.)\":vz-ja9vpp 32vt-7,c65ut2\"f269el:87oma\",pyjzvkg6)x'1k\"lhq0!?3:kp\n",
            "m dd-yua2hz\n",
            ":sv9'vuc9bhx64nc'h,0-(u:'012f3rup4t8,eu6k[qlxte-[17\n",
            "5si\"d6!\"em ]0.5\n",
            "954,2jii!j:dy265l\n",
            "r8ni-!zjsgm6ko z, 5.,1ix,s7n8ateqz:,5xrcks:2f1v\n",
            "8)cjtejcmqmoth(.83u,heb5[\"nlyagt?swt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-foCb1GF721",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}